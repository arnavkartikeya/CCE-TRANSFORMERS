{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e86e07-6989-42b7-8d62-e3cd11591d39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/compexp/nli/code\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09ced10c-c88c-4cac-a60b-e8d91b429d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from data.snli import SNLI, pad_collate\n",
    "from contextlib import nullcontext\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import models\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87220f72-ab4d-4665-83fb-469b43d89a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(split, epoch, model, optimizer, criterion, dataloaders, args):\n",
    "    training = split == \"train\"\n",
    "    if training:\n",
    "        ctx = nullcontext\n",
    "        model.train()\n",
    "    else:\n",
    "        ctx = torch.no_grad\n",
    "        model.eval()\n",
    "\n",
    "    ranger = tqdm(dataloaders[split], desc=f\"{split} epoch {epoch}\")\n",
    "\n",
    "    loss_meter = util.AverageMeter()\n",
    "    acc_meter = util.AverageMeter()\n",
    "    for (s1, s1len, s2, s2len, targets) in ranger:\n",
    "\n",
    "        if args.cuda:\n",
    "            s1 = s1.cuda()\n",
    "            s1len = s1len.cuda()\n",
    "            s2 = s2.cuda()\n",
    "            s2len = s2len.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "        batch_size = targets.shape[0]\n",
    "\n",
    "        with ctx():\n",
    "            logits = model(s1, s1len, s2, s2len)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "        if training:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        preds = logits.argmax(1)\n",
    "        acc = (preds == targets).float().mean()\n",
    "\n",
    "        loss_meter.update(loss.item(), batch_size)\n",
    "        acc_meter.update(acc.item(), batch_size)\n",
    "\n",
    "        ranger.set_description(\n",
    "            f\"{split} epoch {epoch} loss {loss_meter.avg:.3f} acc {acc_meter.avg:.3f}\"\n",
    "        )\n",
    "\n",
    "    return {\"loss\": loss_meter.avg, \"acc\": acc_meter.avg}\n",
    "\n",
    "\n",
    "def build_model(vocab_size, model_type, vocab=None, bert=False, embedding_dim=300, hidden_dim=512):\n",
    "    \"\"\"\n",
    "    Build a bowman-style SNLI model\n",
    "    \"\"\"\n",
    "    if bert:\n",
    "        if vocab is None:\n",
    "            raise Exception('Bert model requires passing the datasets vocab field')\n",
    "        model = models.BertEntailmentClassifier(vocab=vocab,freeze_bert=True)\n",
    "        return model\n",
    "    enc = models.TextEncoder(\n",
    "        vocab_size, embedding_dim=embedding_dim, hidden_dim=hidden_dim\n",
    "    )\n",
    "    if model_type == \"minimal\":\n",
    "        model = models.EntailmentClassifier(enc)\n",
    "    else:\n",
    "        model = models.BowmanEntailmentClassifier(enc)\n",
    "    return model\n",
    "\n",
    "def serialize(model, dataset):\n",
    "    return {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"stoi\": dataset.stoi,\n",
    "        \"itos\": dataset.itos,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d13dc57-ce41-456d-8f3f-45aa6de0aa47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 0it [00:00, ?it/s]/opt/conda/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "train: 1000it [00:04, 205.32it/s]\n",
      "dev: 1000it [00:04, 207.84it/s]\n"
     ]
    }
   ],
   "source": [
    "max_data = 1000\n",
    "train = SNLI(\"../data/snli_1.0/\", \"train\", max_data=max_data)\n",
    "val = SNLI(\n",
    "    \"../data/snli_1.0/\", \"dev\", max_data=max_data, vocab=(train.stoi, train.itos)\n",
    ")\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(\n",
    "        train,\n",
    "        batch_size=100,\n",
    "        shuffle=True,\n",
    "        pin_memory=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=pad_collate,\n",
    "    ),\n",
    "    \"val\": DataLoader(\n",
    "        val,\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=pad_collate,\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1816034a-4d15-4e0b-b2f6-05344083d51a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 100]) torch.Size([20, 100])\n",
      "torch.Size([100, 3])\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    len(train.stoi),\n",
    "    'doesnt matter',\n",
    "    {'stoi': train.stoi, 'itos': train.itos}, \n",
    "    True, \n",
    ")\n",
    "model = model.to('cuda')\n",
    "\n",
    "for (s1, s1len, s2, s2len, targets) in dataloaders['train']: \n",
    "    s1 = s1.to('cuda')\n",
    "    s1len = s1len.to('cuda')\n",
    "    s2 = s2.to('cuda')\n",
    "    \n",
    "    print(s1.shape, s2.shape)\n",
    "    s2len = s2len.to('cuda')\n",
    "    targets = targets.to('cuda')\n",
    "    outputs = model(s1, s1len, s2, s2len)\n",
    "    break  # Just for testing the first batch\n",
    "\n",
    "print(outputs.shape)  # This should work without CUDA errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d288cc86-8b4b-4b37-bd25-2e2933006ce8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ckpt = torch.load('models/snli/0.pth', map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8daf8f75-1281-46c8-ab81-f6a7f6437665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['state_dict', 'stoi', 'itos'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ad2fcb-bc3b-469c-9f8d-8c4c385396c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu121.m122",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu121:m122"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
